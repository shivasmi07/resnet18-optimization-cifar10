{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Device set up\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# load the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    " \n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# set up the network\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise Network\n",
    "ResNet18 = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "ResNet18.load_state_dict(torch.load('GD.pkl'))\n",
    "ResNet18.linear.reset_parameters()\n",
    "\n",
    "# Number of parameters in the last layer of network\n",
    "num_of_weights = sum(p.numel() for p in ResNet18.linear.parameters())\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "# Fitness function setup (minimize loss)\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) # -1 is for minimise\n",
    "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
    "\n",
    "posMinInit      = -2\n",
    "posMaxInit      = +2\n",
    "VMaxInit        = 1.5\n",
    "VMinInit        = 0.5\n",
    "populationSize  = 30\n",
    "dimension       = num_of_weights\n",
    "interval        = 10\n",
    "iterations      = 50 \n",
    "\n",
    "#Parameter setup\n",
    "wmax = 0.9 #weighting\n",
    "wmin = 0.4 \n",
    "c1   = 2.0\n",
    "c2   = 2.0\n",
    "\n",
    "def generate(size, smin, smax):\n",
    "    part = creator.Particle(random.uniform(posMinInit, posMaxInit) for _ in range(size)) \n",
    "    part.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(size)]\n",
    "    part.smin = smin\n",
    "    part.smax = smax\n",
    "    return part\n",
    "\n",
    "\n",
    "def updateParticle(part, best, weight):\n",
    "\n",
    "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "\n",
    "    v_r0 = [weight*x for x in part.speed]\n",
    "    v_r1 = [c1*x for x in map(operator.mul, r1, map(operator.sub, part.best, part))] # local best\n",
    "    v_r2 = [c2*x for x in map(operator.mul, r2, map(operator.sub, best, part))] # global best\n",
    "    \n",
    "    part.speed = [0.7*x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
    "            \n",
    "    # update position with speed\n",
    "    part[:] = list(map(operator.add, part, part.speed))\n",
    "    \n",
    "def fitness(part):\n",
    "    weights = np.asarray(part)\n",
    "    ResNet18.linear.weight = torch.nn.Parameter(torch.from_numpy(weights[0:5120].reshape(512, 10).T).float())  # Update last layer weights\n",
    "    ResNet18.linear.bias = torch.nn.Parameter(torch.from_numpy(weights[5120:5130].flatten()).float())\n",
    "    ResNet18.to(device)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = ResNet18(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(trainloader)\n",
    "    loss_values.append(avg_loss)\n",
    "    return (avg_loss,)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"particle\", generate, size=dimension, smin=-3, smax=3)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
    "toolbox.register(\"update\", updateParticle)\n",
    "toolbox.register(\"evaluate\", fitness) \n",
    "\n",
    "fitness_best = []\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    pop = toolbox.population(n=populationSize)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "\n",
    "    best = None\n",
    "    \n",
    "    #begin main loop\n",
    "    for g in range(iterations):\n",
    "        w = wmax - (wmax-wmin)*g/iterations #decaying inertia weight\n",
    "        \n",
    "        for part in pop:\n",
    "            part.fitness.values = toolbox.evaluate(part)\n",
    "            \n",
    "            #update local best\n",
    "            if (not part.best) or (part.best.fitness < part.fitness):    \n",
    "                part.best = creator.Particle(part)\n",
    "                part.best.fitness.values = part.fitness.values\n",
    "            \n",
    "            #update global best\n",
    "            if (not best) or best.fitness < part.fitness:\n",
    "                best = creator.Particle(part)\n",
    "                best.fitness.values = part.fitness.values\n",
    "                \n",
    "        for part in pop:\n",
    "            toolbox.update(part, best,w)\n",
    "        \n",
    "        fitness_best.append(best.fitness.values)\n",
    "        print('completed',g)\n",
    "        \n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "        # print every interval\n",
    "        if g%interval==0:\n",
    "            logbook.record(gen=g, evals=len(pop), **stats.compile(pop))\n",
    "            print(logbook.stream)\n",
    "            print('best ', best.fitness)\n",
    "    \n",
    "    print('fitness of best is', best.fitness)\n",
    "    return pop, logbook, best\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pop, logbook, best = main()\n",
    "\n",
    "weights = np.asarray(best)\n",
    "ResNet18.linear.weight = torch.nn.Parameter(torch.from_numpy(weights[0:5120].reshape(512, 10).T).float())  # Update last layer weights\n",
    "ResNet18.linear.bias = torch.nn.Parameter(torch.from_numpy(weights[5120:5130].flatten()).float())\n",
    "\n",
    "#save the network\n",
    "torch.save(ResNet18.state_dict(), 'PSO.pkl') \n",
    "\n",
    "#plot the loss\n",
    "plt.plot(np.array(fitness_best), 'r')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('PSO_loss.pdf')\n",
    "\n",
    "# Save to a CSV file\n",
    "with open('PSO_fitness.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fitness_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNet = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "testNet.load_state_dict(torch.load('PSO.pkl'))\n",
    "\n",
    "total = 0\n",
    "correct= 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x, y = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = testNet(x)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "\n",
    "accuracy = correct/total\n",
    "print('Accurarcy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
